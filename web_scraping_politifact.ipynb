{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scraping_Politifact.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgScpbI4Gs-v",
        "outputId": "3cc474cb-4935-4563-ebf0-e5dcd225c6e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "#result = requests.get(\"https://www.politifact.com/factchecks/list/\")\n",
        "\n",
        "#print(result.status_code) #if 200, then page is accessible\n",
        "#print(result.headers)\n",
        "\n",
        "#src = result.content\n",
        "\n",
        "#soup = BeautifulSoup(src, features=\"html.parser\")\n",
        "#print(soup.prettify())\n",
        "quotes = []\n",
        "for page in range(1,599): #go search the website how many pages are there\n",
        "  print('processing page :', page)\n",
        "  url = 'https://www.politifact.com/factchecks/list/?page='+str(page)+'&'\n",
        "  print(url)\n",
        "    \n",
        "    #an exception might be thrown, so the code should be in a try-except block\n",
        "  try:\n",
        "        #use the browser to get the url. This is suspicious command that might blow up.\n",
        "      page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
        "    \n",
        "  except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "      continue                                              #ignore this page. Abandon this and go back.\n",
        "  time.sleep(2)   \n",
        "  src = page.content\n",
        "  soup=BeautifulSoup(src,'html.parser')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  total_article = soup.find_all('li', attrs = {'class':'o-listicle__item'})\n",
        "  \n",
        "  for link in total_article:   \n",
        "    news = {}\n",
        "    \n",
        "    try:    \n",
        "      news_desc = link.find(\"div\",attrs={'class':'m-statement__quote'}).find('a').text.strip()\n",
        "      news['News'] = news_desc\n",
        "\n",
        "      source_desc = link.find(\"div\", attrs={'class' : 'm-statement__meta'}).find('a')['title'].strip()\n",
        "      news['Source / Said By'] = source_desc\n",
        "        \n",
        "      writter_desc = link.find(\"div\", attrs={'class' : 'm-statement__body'}).find('footer').text.split(\"By\")[1].split('•')[0].strip()\n",
        "      news['Writter'] = writter_desc\n",
        "\n",
        "      published_desc = link.find(\"div\", attrs={'class' : 'm-statement__body'}).find('footer').text.split(\"By\")[1].split('•')[1].strip()\n",
        "      news['Published on'] = published_desc\n",
        "\n",
        "      stated_on_desc = link.find(\"div\", attrs={'class' : 'm-statement__desc'}).text.split(\"on\")[1].split(\"in\")[0].strip()\n",
        "    \n",
        "      \n",
        "      stated_on_desc1 = link.find(\"div\", attrs={'class' : 'm-statement__desc'}).text.split(\"on\")[1].split(\"in\")[1].strip() \n",
        "      news['stated_on'] = stated_on_desc\n",
        "      news['Via'] = stated_on_desc1\n",
        "      result_desc = link.find(\"div\", attrs={'class' : 'm-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
        "      news['Classification'] = result_desc\n",
        "    except IndexError as error:\n",
        "      continue \n",
        "\n",
        "\n",
        "\n",
        "    quotes.append(news)\n",
        "\n",
        "  filename = 'news_scrap2.csv'\n",
        "  with open(filename, 'w', newline='') as f: \n",
        "      w = csv.DictWriter(f, ['News', 'Source / Said By', 'stated_on', 'Via', 'Writter', 'Published on', 'Classification'])\n",
        "      w.writeheader() \n",
        "      for quote in quotes: \n",
        "          w.writerow(quote)\n",
        "                         \n",
        "          \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing page : 1\n",
            "https://www.politifact.com/factchecks/list/?page=1&\n",
            "processing page : 2\n",
            "https://www.politifact.com/factchecks/list/?page=2&\n",
            "processing page : 3\n",
            "https://www.politifact.com/factchecks/list/?page=3&\n",
            "processing page : 4\n",
            "https://www.politifact.com/factchecks/list/?page=4&\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67IZ_kz6lu1Q"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}